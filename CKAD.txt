

Lab 1: Jobs and Cronjobs in Kubernetes

Task 1: Jobs in kubernetes 
 
apiVersion: batch/v1
kind: Job
metadata:
  name: jobs-hello
spec:
  template:
    metadata:
      name: jobs-pod
    spec:
      containers:
      - name: jobs-ctr
        image: busybox
        args:
        - /bin/sh
        - -c
        - echo HELLO WORLD !!!!!
      restartPolicy: Never

kubectl create -f job.yaml
kubectl get jobs
kubectl get pods

read logs 
kubectl logs jobs-hello-7cfn2 
HELLO WORLD !!!!!

Describe job to see
kubectl describe jobs jobs-hello

kubectl delete -f job.yaml

Task 2: Cronjobs 
1.	Create a yaml called cron.yaml. Use the content given below to fill the file

vi cron.yaml
 
apiVersion: batch/v1
kind: CronJob
metadata:
  name: cronjob-hello
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: cronjob-ctr
            image: busybox
            args:
            - /bin/sh
            - -c
            - echo Hello World!
          restartPolicy: OnFailure

kubectl create -f cronjob.yaml

kubectl get cronjobs.batch 
NAME            SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob-hello   */1 * * * *   False     0        28s             42s

kubectl get pod
NAME                           READY   STATUS      RESTARTS   AGE
cronjob-hello-27991419-lz69c   0/1     Completed   0          42s

kubectl logs cronjob-hello-27991419-lz69c 
Hello World!



Lab 2: Sidecar container and Patterns

Task: 1 sidecar container

vi sidecar.yaml

apiVersion: v1
kind: Pod
metadata:
  name: pod-sidecar
spec:
  containers:
  - name: app-container
    image: ubuntu:latest
    command: ["/bin/sh"]
    args: ["-c","while true; do date >> /var/log/app.txt; sleep 5; done"]
    volumeMounts:
    - name: share-logs
      mountPath: /var/log/
  - name: sidecar-container
    image: nginx:latest
    ports:
    - containerPort: 80
    volumeMounts:
    - name: share-logs
      mountPath: /usr/share/nginx/html
  volumes:
  - name: share-logs
    emptyDir: {}
	
	
kubectl create -f sidecar.yaml
kubectl get pod

kubectl exec -it pod-sidecar -c sidecar-container -- bash
install curl
apt update && apt install curl -y
 
curl 'http://localhost:80/app.txt'
 

Lab 3 Canary Deployment on Kubernetes 

vi web-blue.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-blue
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web-blue
      type: web-app
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: web-blue
        type: web-app
    spec:
      containers:
      - image: mandarct/web-blue:v1
        name: web-blue
        ports:
        - containerPort: 80
          protocol: TCP
		 
kubectl create -f web-blue.yaml
kubectl get deploy
kubectl get pods

Now create NodePort service to access application

		 
vi svc-web-lb.yaml

apiVersion: v1
kind: Service
metadata:
  name: web-app-svc-lb
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
  selector:
    type: web-app
  type: NodePort
  ports:
   - port: 80
     targetPort: 80
	 
kubectl create -f svc-web-lb.yaml
kubectl get svc

access you application


to it canary deployment deploy following yaml file

vi web-green.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-green
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web-green
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: web-green
        type: web-app
    spec:
      containers:
      - image: mandarct/web-green:v1
        name: web-green
        ports:
        - containerPort: 80
          protocol: TCP

kubectl create -f web-green.yaml
kubectl get deployment
kubectl get pods

access this application using same service that we create previously.

some time will to blue deployment web page, some time green webpage

If you delete the web-green deployment, load-balancer will start sending traffic only to the blue pods


kubectl describe svc web-app-svc-lb


**** Helm Chart ****

Installing Helm 3 in Ubuntu Linux and installing wordpress (LAMP) application through Helm.

wget files.cloudthat.training/devops/kubernetes-essentials/helm.sh
cat helm.sh
bash helm.sh
helm version
helm repo add bitnami https://charts.bitnami.com/bitnami
helm repo update
helm repo list
helm search repo wordpress
helm install wordpress-chart bitnami/wordpress
Helm ls
helm uninstall wordpress-chart


Liveness, Readiness and Startup Probes
•	Exec
•	TCPSocket
•	HTTPGetAction 
		  
Lab 4: Liveness and Readiness probes
		  
vi liveness-pod.yaml

apiVersion: v1
kind: Pod
metadata:
  labels:
    app: lns
  name: liveness-pod
spec:
  containers:
  - name: liveness
    image: busybox
    args:
    - /bin/sh
    - -c
    - touch /liveness; sleep 6000
    livenessProbe:
      exec:
        command:
        - cat
        - /liveness
      initialDelaySeconds: 5
      periodSeconds: 5
	  
kubectl get pods	  
kubectl describe pod liveness-exec
 
now login to container and delete following file
kubectl exec -it liveness-pod sh 
# rm -f /liveness
# exit

kubectl get pods
you can see pod is getting restarted

 

vi readiness.yaml

apiVersion: v1
kind: Pod
metadata:
  labels:
    app: rns
  name: readiness-pod
spec:
  containers:
  - name: readiness
    image: nginx
    args:
    - /bin/bash
    - -c
    - service nginx start; touch /readiness; sleep 6000
    readinessProbe:
      exec:
        command:
        - cat
        - /readiness
      initialDelaySeconds: 5
      periodSeconds: 5
	  
kubectl create -f readiness.yaml
kubectl get pods


login inside pod and delete folloing file 

kubectl exec -it readiness-pod bash 
# rm -f /readiness
# exit


View the Pod events and see that readiness probe has failed
kubectl describe pods readiness-pod

now describe service
kubectl describe svc readiness-svc

End point is gone


Lab 5: Resource Quotas in Kubernetes

Task 1: Creating a Namespace
kubectl create namespace quotas
kubectl get ns
kubectl describe ns quotas

Task 2: Creating a resourcequota

vi rq-quotas.yaml

apiVersion: v1
kind: ResourceQuota
metadata:
  name: quota
  namespace: quotas
spec:
  hard:
    requests.cpu: "1"
    requests.memory: 1Gi
    limits.cpu: "2"
    limits.memory: 2Gi

kubectl create -f rq-quotas.yaml
kubectl describe ns resourcequota




Task 3: Verify resourcequota Functionality

vi rq-pod.yaml

apiVersion: v1
kind: Pod
metadata:
  name: quota-pod
  namespace: quotas
spec:
  containers:
  - name: quota-ctr
    image: nginx
    resources:
      limits:
        memory: "800Mi"
        cpu: "1000m"
      requests:
        memory: "600Mi"
        cpu: "350m"
    ports:
      - containerPort: 80
	  
kubectl create -f rq-pod.yaml
kubectl describe ns resourcequota
kubectl get resourcequota -n quotas -o yaml


Task 4: Limiting Number of Pods

kubectl edit resourcequotas quota -n quotas
add following lines in spec: > hard:
count/deployments.apps: 2
count/replicasets.apps: 3

Task 5: Clean-up
1.	Delete the quota to clean up.
$ kubectl delete ns quotas


Lab 6: ConfigMap
Task 1: Setting container environment variables using configmap

imperative way to create config-map in environment variables
kubectl create configmap my-configmap --from-literal mydata=hello_world --dry-run=client -o yaml

vi config-map.yaml

apiVersion: v1
data:
  mydata: hello_world
kind: ConfigMap
metadata:
  name: my-configmap

kubectl create -f config-map.yaml
 kubectl get configmaps


vi cm-pod.yaml

apiVersion: v1
kind: Pod
metadata:
  name: cm-pod
spec:
  containers:
  - name: quota-ctr
    image: nginx:latest
    ports:
    - containerPort: 80
    env:
    - name: hlo
      valueFrom:
        configMapKeyRef:
           name: my-configmap
           key: mydata
		   
kubectl exec -it cm-pod -- bash
echo $hlo
hello_world
exit


Task 2: Setting configuration file with volume using ConfigMap

vi redis-cm.yaml
 
apiVersion: v1
data:
  redis-config: |
    maxmemory 2mb
    maxmemory-policy allkeys-lru
kind: ConfigMap
metadata:
  name: example-redis-config
  namespace: default

kubectl create -f redis-cm.yaml 
kubectl get cm
kubectl describe cm example-redis-config 

Create pod

vi redis-pod.yaml

apiVersion: v1
kind: Pod
metadata:
  name: redis
spec:
  containers:
  - name: redis
    image: redis
    env:
    - name: MASTER
      value: "true"
    ports:
    - containerPort: 6379
    volumeMounts:
    - mountPath: /redis-master-data
      name: data
    - mountPath: /redis-master
      name: config
  volumes:
    - name: data
      emptyDir: {}
    - name: config
      configMap:
        name: example-redis-config
        items:
        - key: redis-config
          path: redis.conf

kubectl create -f redis-pod.yaml
read config map's file 
kubectl exec -it redis cat /redis-master/redis.conf

Task 3: Creating a Secret

vi secrets.yaml

apiVersion: v1
kind: Secret
metadata:
  name: mysql-credentials
type: Opaque
data:
  ##all below values are base64 encoded
  ##rootpw is root
  ##user is user
  ##password is mypwd
  rootpw: cm9vdAo=
  user: dXNlcgo=
  password: bXlwd2QK
  
kubectl create -f secrets.yaml
kubectl get secrets
kubectl describe secrets mysql-credentials
see your encoded secrets 
kubectl get secrets mysql-credentials -o yaml


Lab 7: Pod and Container level security using Context

Task 1: Set the security context for a pod

apiVersion: v1
kind: Pod
metadata:
  name: sc-pod
spec:
  securityContext:
    runAsUser: 1000
    runAsGroup: 3000
    fsGroup: 2000
  volumes:
    - name: sc-vol
      emptyDir: {}
  containers:
    - name: sc-ctr
      image: busybox
      command:
        - sh
        - -c
        - sleep 1h
      volumeMounts:
        - name: sc-vol
          mountPath: /data/demo
      securityContext:
        allowPrivilegeEscalation: false
  
 kubectl create -f sc-pod.yml
 kubectl exec -it sc-pod -- sh
 ps 
 cd /data
 ls
 
cd demo
echo hello > testfile
ls -l


Task 2: Set the security context for a container

vi sc-ctr.yml
---
apiVersion: v1
kind: Pod
metadata:
  name: sc-pod2
spec:
  securityContext:
    runAsUser: 1000
  containers:
  - name: sc-ctr2
    image: gcr.io/google-samples/node-hello:1.0
    securityContext:
      runAsUser: 2000
      allowPrivilegeEscalation: false
	  
kubectl exec -it sc-pod2 -- sh
ps aux




Lab 8: Kubernetes Network Policy

kubectl run backend --image=nginx --labels role=backend --expose --port=80

kubectl get pod
kubectl get svc


kubectl run --rm -it --image=busybox net-policy 
wget -qO- -T3 http://backend
exit

3.	Create a network policy which uses labels to deny all ingress traffic

vi np-deny-all.yml

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: backend-policy
spec:
  podSelector:
    matchLabels:
      role: backend
  ingress: []

kubectl create -f np-deny-all.yml
kubectl get networkpolicies

4.	Create a new busybox pod again and verify that it cannot access the backend service

kubectl run --rm -it --image=busybox net-policy
wget -qO- -T3 http://backend
it will show timeout
exit



5.	Modify the network policy to allow traffic with matching Pod labels 
Inspect the network policy and notice the selectors

vi np-pod-label-allow.yml

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: backend-policy
spec:
  podSelector:
    matchLabels:
      role: backend
  ingress:
  - from:
    - namespaceSelector: {}
      podSelector:
        matchLabels:
          role: frontend
		  

before applyinh this yaml describe networkpolicies to check rule

kubectl describe networkpolicies backend-policy

kubectl apply -f np-pod-label-allow.yml

kubectl describe networkpolicies backend-policy


kubectl run --rm -it --image=busybox --labels role=frontend net-policy
wget -qO- -T3 http://backend
exit


Lab 9: Node Labelling and Constraining pods in Kubernetes

Task 1: Deploy pod in specific node based on node-name / hostname

vi node-name.yaml

apiVersion: v1
kind: Pod
metadata:
  name: nginx-nodename
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  nodeName: {node-name / hostname }

kubectl create -f node-name.yaml
kubectl get pods -o wide 
see your pod in defined node
 

Task 1.1: Node labeling and constraining pods

list node labels
kubectl get nodes --show-labels

kubectl label nodes <node_name> disktype=ssd

list you nodes to check labels 
kubectl get nodes --show-labels | grep "disktype=ssd"

vi nlns-pod.yaml

apiVersion: v1
kind: Pod
metadata:
  name: nlns-nginx-pod
  labels:
    env: test
spec:
  containers:
  - name: nlns-nginx-ctr
    image: nginx
  nodeSelector:
    disktype: ssd


kubectl apply -f nlns-pod.yaml
kubectl get pods -o wide

Lab 10: Advanced Pod Scheduling

Task 1: Node Affinity

vi node-aff-pref.yaml

apiVersion: v1
kind: Pod
metadata:
  name: na-nginx-pod1
spec:
  containers:
  - name: na-nginx-ctr1
    image: nginx
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd

kubectl apply -f node-aff-pref.yaml
kubectl get pods -o wide

Remove node label 

kubectl label nodes <node_name> disktype-
deploy same pod again it is getting deployed

Task 2: required during scheduling

remove label first 
kubectl label nodes <node_name> disktype-


vi aff-na-pod2.yaml

apiVersion: v1
kind: Pod
metadata:
  name: na-nginx-pod2
spec:
  containers:
  - name: na-nginx-ctr2
    image: nginx
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd

kubectl create -f aff-na-pod2.yaml

pod is strictly looking for labeled node

kubectl get pods -o wide
kubectl label nodes <node_name> disktype=ssd

once label is added pod will deployed


Task 2: Pod Affinity

vi depend-pod.yaml

apiVersion: v1
kind: Pod
metadata:
  name: pa-nginx-pod1
  labels:
    app: pa-nginx-app
spec:
  containers:
  - name: pa-nginx-ctr1
    image: nginx

kubectl create -f depend-pod.yaml
kubectl get pods

Task 3: create pod with pod affinity

vi aff-pa-pod2.yaml

apiVersion: v1
kind: Pod
metadata:
  name: pa-nginx-pod2
spec:
  containers:
  - name: pa-nginx-ctr2
    image: nginx
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - pa-nginx-app
        topologyKey: kubernetes.io/hostname

kubectl create -f pod-affinity.yaml

kubectl get pods -o wide
both pod are in same node

Task 4: pod anti affinity

vi pod-antiaff.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - pa-nginx-app
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: nginx
        image: nginx

kubectl create -f  pod-antiaff.yaml
kubectl get pod -o wide 


kubectl delete -f  pod-antiaff.yaml
 









